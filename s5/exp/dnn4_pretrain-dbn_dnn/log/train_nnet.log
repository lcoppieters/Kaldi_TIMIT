# Running on vgnd012
# Started at Tue Oct 20 12:28:36 CEST 2020
# steps/nnet/train.sh --feature-transform exp/dnn4_pretrain-dbn/final.feature_transform --dbn exp/dnn4_pretrain-dbn/6.dbn --hid-layers 0 --learn-rate 0.008 data-fmllr-tri3/train_tr90 data-fmllr-tri3/train_cv10 data/lang exp/tri3_ali exp/tri3_ali exp/dnn4_pretrain-dbn_dnn 
steps/nnet/train.sh --feature-transform exp/dnn4_pretrain-dbn/final.feature_transform --dbn exp/dnn4_pretrain-dbn/6.dbn --hid-layers 0 --learn-rate 0.008 data-fmllr-tri3/train_tr90 data-fmllr-tri3/train_cv10 data/lang exp/tri3_ali exp/tri3_ali exp/dnn4_pretrain-dbn_dnn

# INFO
steps/nnet/train.sh : Training Neural Network
	 dir       : exp/dnn4_pretrain-dbn_dnn 
	 Train-set : data-fmllr-tri3/train_tr90 3328, exp/tri3_ali 
	 CV-set    : data-fmllr-tri3/train_cv10 368 exp/tri3_ali 

LOG ([5.5.734~1-794732a]:main():cuda-gpu-available.cc:60) 

### IS CUDA GPU AVAILABLE? 'vgnd012' ###
WARNING ([5.5.734~1-794732a]:SelectGpuId():cu-device.cc:228) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG ([5.5.734~1-794732a]:SelectGpuIdAuto():cu-device.cc:408) Selecting from 1 GPUs
LOG ([5.5.734~1-794732a]:SelectGpuIdAuto():cu-device.cc:423) cudaSetDevice(0): Tesla K80	free:11372M, used:68M, total:11441M, free/total:0.993991
LOG ([5.5.734~1-794732a]:SelectGpuIdAuto():cu-device.cc:471) Device: 0, mem_ratio: 0.993991
LOG ([5.5.734~1-794732a]:SelectGpuId():cu-device.cc:352) Trying to select device: 0
LOG ([5.5.734~1-794732a]:SelectGpuIdAuto():cu-device.cc:481) Success selecting device 0 free mem ratio: 0.993991
LOG ([5.5.734~1-794732a]:FinalizeActiveGpu():cu-device.cc:308) The active GPU is [0]: Tesla K80	free:11300M, used:140M, total:11441M, free/total:0.987698 version 3.7
### HURRAY, WE GOT A CUDA GPU FOR COMPUTATION!!! ##

### Testing CUDA setup with a small computation (setup = cuda-toolkit + gpu-driver + kaldi):
### Test OK!

# PREPARING ALIGNMENTS
Using PDF targets from dirs 'exp/tri3_ali' 'exp/tri3_ali'
hmm-info exp/tri3_ali/final.mdl 
copy-transition-model --binary=false exp/tri3_ali/final.mdl exp/dnn4_pretrain-dbn_dnn/final.mdl 
LOG (copy-transition-model[5.5.734~1-794732a]:main():copy-transition-model.cc:62) Copied transition model.

# PREPARING FEATURES
# re-saving features to local disk,
copy-feats --compress=true scp:data-fmllr-tri3/train_tr90/feats.scp ark,scp:/tmp/kaldi.x4Mr/train.ark,exp/dnn4_pretrain-dbn_dnn/train_sorted.scp 
LOG (copy-feats[5.5.734~1-794732a]:main():copy-feats.cc:143) Copied 3328 feature matrices.
copy-feats --compress=true scp:data-fmllr-tri3/train_cv10/feats.scp ark,scp:/tmp/kaldi.x4Mr/cv.ark,exp/dnn4_pretrain-dbn_dnn/cv.scp 
LOG (copy-feats[5.5.734~1-794732a]:main():copy-feats.cc:143) Copied 368 feature matrices.
# importing feature settings from dir 'exp/dnn4_pretrain-dbn'
# cmvn_opts='' delta_opts='' ivector_dim=''
# 'apply-cmvn' is not used,
feat-to-dim 'ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- |' - 
copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- 
WARNING (feat-to-dim[5.5.734~1-794732a]:Close():kaldi-io.cc:515) Pipe copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- | had nonzero return status 36096
# feature dim : 40 (input of 'feature_transform')
# importing 'feature_transform' from 'exp/dnn4_pretrain-dbn/final.feature_transform'

### Showing the final 'feature_transform':
nnet-info exp/dnn4_pretrain-dbn_dnn/imported_final.feature_transform 
num-components 3
input-dim 40
output-dim 440
number-of-parameters 0.00088 millions
component 1 : <Splice>, input-dim 40, output-dim 440, 
  frame_offsets [ -5 -4 -3 -2 -1 0 1 2 3 4 5 ]
component 2 : <AddShift>, input-dim 440, output-dim 440, 
  shift_data ( min -0.115547, max 0.175501, mean -0.00607088, stddev 0.0387011, skewness 1.43335, kurtosis 9.0895 ) , lr-coef 0
component 3 : <Rescale>, input-dim 440, output-dim 440, 
  scale_data ( min 0.317607, max 0.968554, mean 0.764179, stddev 0.156348, skewness -0.661368, kurtosis -0.181612 ) , lr-coef 0
LOG (nnet-info[5.5.734~1-794732a]:main():nnet-info.cc:57) Printed info about exp/dnn4_pretrain-dbn_dnn/imported_final.feature_transform
###

# NN-INITIALIZATION
# getting input/output dims :
feat-to-dim 'ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- | nnet-forward "nnet-concat exp/dnn4_pretrain-dbn_dnn/final.feature_transform '\''exp/dnn4_pretrain-dbn/6.dbn'\'' -|" ark:- ark:- |' - 
copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- 
nnet-forward "nnet-concat exp/dnn4_pretrain-dbn_dnn/final.feature_transform 'exp/dnn4_pretrain-dbn/6.dbn' -|" ark:- ark:- 
LOG (nnet-forward[5.5.734~1-794732a]:SelectGpuId():cu-device.cc:153) Manually selected to compute on CPU.
nnet-concat exp/dnn4_pretrain-dbn_dnn/final.feature_transform exp/dnn4_pretrain-dbn/6.dbn - 
LOG (nnet-concat[5.5.734~1-794732a]:main():nnet-concat.cc:53) Reading exp/dnn4_pretrain-dbn_dnn/final.feature_transform
LOG (nnet-concat[5.5.734~1-794732a]:main():nnet-concat.cc:65) Concatenating exp/dnn4_pretrain-dbn/6.dbn
LOG (nnet-concat[5.5.734~1-794732a]:main():nnet-concat.cc:82) Written model to -
WARNING (feat-to-dim[5.5.734~1-794732a]:Close():kaldi-io.cc:515) Pipe copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp.10k ark:- | nnet-forward "nnet-concat exp/dnn4_pretrain-dbn_dnn/final.feature_transform 'exp/dnn4_pretrain-dbn/6.dbn' -|" ark:- ark:- | had nonzero return status 36096
# genrating network prototype exp/dnn4_pretrain-dbn_dnn/nnet.proto
# initializing the NN 'exp/dnn4_pretrain-dbn_dnn/nnet.proto' -> 'exp/dnn4_pretrain-dbn_dnn/nnet.init'
nnet-initialize --seed=777 exp/dnn4_pretrain-dbn_dnn/nnet.proto exp/dnn4_pretrain-dbn_dnn/nnet.init 
VLOG[1] (nnet-initialize[5.5.734~1-794732a]:Init():nnet-nnet.cc:314) <AffineTransform> <InputDim> 1024 <OutputDim> 1928 <BiasMean> 0.000000 <BiasRange> 0.000000 <ParamStddev> 0.091101
VLOG[1] (nnet-initialize[5.5.734~1-794732a]:Init():nnet-nnet.cc:314) <Softmax> <InputDim> 1928 <OutputDim> 1928
VLOG[1] (nnet-initialize[5.5.734~1-794732a]:Init():nnet-nnet.cc:314) </NnetProto>
LOG (nnet-initialize[5.5.734~1-794732a]:main():nnet-initialize.cc:63) Written initialized model to exp/dnn4_pretrain-dbn_dnn/nnet.init
nnet-concat exp/dnn4_pretrain-dbn/6.dbn exp/dnn4_pretrain-dbn_dnn/nnet.init exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init 
LOG (nnet-concat[5.5.734~1-794732a]:main():nnet-concat.cc:53) Reading exp/dnn4_pretrain-dbn/6.dbn
LOG (nnet-concat[5.5.734~1-794732a]:main():nnet-concat.cc:65) Concatenating exp/dnn4_pretrain-dbn_dnn/nnet.init
LOG (nnet-concat[5.5.734~1-794732a]:main():nnet-concat.cc:82) Written model to exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init

# RUNNING THE NN-TRAINING SCHEDULER
steps/nnet/train_scheduler.sh --feature-transform exp/dnn4_pretrain-dbn_dnn/final.feature_transform --learn-rate 0.008 exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp ark:- | ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/cv.scp ark:- | ark:ali-to-pdf exp/tri3_ali/final.mdl "ark:gunzip -c exp/tri3_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | ark:ali-to-pdf exp/tri3_ali/final.mdl "ark:gunzip -c exp/tri3_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | exp/dnn4_pretrain-dbn_dnn
CROSSVAL PRERUN AVG.LOSS 7.7208 (Xent),
ITERATION 01: TRAIN AVG.LOSS 2.1068, (lrate0.008), CROSSVAL AVG.LOSS 1.9075, nnet accepted (nnet_dbn_dnn_iter01_learnrate0.008_tr2.1068_cv1.9075)
ITERATION 02: TRAIN AVG.LOSS 1.4029, (lrate0.008), CROSSVAL AVG.LOSS 1.7852, nnet accepted (nnet_dbn_dnn_iter02_learnrate0.008_tr1.4029_cv1.7852)
ITERATION 03: TRAIN AVG.LOSS 1.1967, (lrate0.008), CROSSVAL AVG.LOSS 1.7555, nnet accepted (nnet_dbn_dnn_iter03_learnrate0.008_tr1.1967_cv1.7555)
ITERATION 04: TRAIN AVG.LOSS 1.0495, (lrate0.008), CROSSVAL AVG.LOSS 1.7561, nnet rejected (nnet_dbn_dnn_iter04_learnrate0.008_tr1.0495_cv1.7561_rejected)
ITERATION 05: TRAIN AVG.LOSS 1.0058, (lrate0.004), CROSSVAL AVG.LOSS 1.6402, nnet accepted (nnet_dbn_dnn_iter05_learnrate0.004_tr1.0058_cv1.6402)
ITERATION 06: TRAIN AVG.LOSS 0.9154, (lrate0.002), CROSSVAL AVG.LOSS 1.5735, nnet accepted (nnet_dbn_dnn_iter06_learnrate0.002_tr0.9154_cv1.5735)
ITERATION 07: TRAIN AVG.LOSS 0.8754, (lrate0.001), CROSSVAL AVG.LOSS 1.5298, nnet accepted (nnet_dbn_dnn_iter07_learnrate0.001_tr0.8754_cv1.5298)
ITERATION 08: TRAIN AVG.LOSS 0.8575, (lrate0.0005), CROSSVAL AVG.LOSS 1.5023, nnet accepted (nnet_dbn_dnn_iter08_learnrate0.0005_tr0.8575_cv1.5023)
ITERATION 09: TRAIN AVG.LOSS 0.8488, (lrate0.00025), CROSSVAL AVG.LOSS 1.4866, nnet accepted (nnet_dbn_dnn_iter09_learnrate0.00025_tr0.8488_cv1.4866)
ITERATION 10: TRAIN AVG.LOSS 0.8438, (lrate0.000125), CROSSVAL AVG.LOSS 1.4782, nnet accepted (nnet_dbn_dnn_iter10_learnrate0.000125_tr0.8438_cv1.4782)
ITERATION 11: TRAIN AVG.LOSS 0.8405, (lrate6.25e-05), CROSSVAL AVG.LOSS 1.4736, nnet accepted (nnet_dbn_dnn_iter11_learnrate6.25e-05_tr0.8405_cv1.4736)
ITERATION 12: TRAIN AVG.LOSS 0.8384, (lrate3.125e-05), CROSSVAL AVG.LOSS 1.4714, nnet accepted (nnet_dbn_dnn_iter12_learnrate3.125e-05_tr0.8384_cv1.4714)
ITERATION 13: TRAIN AVG.LOSS 0.8371, (lrate1.5625e-05), CROSSVAL AVG.LOSS 1.4704, nnet accepted (nnet_dbn_dnn_iter13_learnrate1.5625e-05_tr0.8371_cv1.4704)
finished, too small rel. improvement 0.000666023
steps/nnet/train_scheduler.sh: Succeeded training the Neural Network : 'exp/dnn4_pretrain-dbn_dnn/final.nnet'
steps/nnet/train.sh: Successfuly finished. 'exp/dnn4_pretrain-dbn_dnn'
# Removing features tmpdir /tmp/kaldi.x4Mr @ vgnd012
cv.ark
train.ark
# Accounting: time=715 threads=1
# Finished at Tue Oct 20 12:40:31 CEST 2020 with status 0
